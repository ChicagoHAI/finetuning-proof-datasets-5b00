idea:
  title: Are there any finetuning proof datasets currently?
  domain: machine_learning
  hypothesis: 'Most datasets become easier for models after fine-tuning due to train/test
    overlap or amplification of known subdistributions. This research asks: Which
    datasets are most resistant to fine-tuning, and are any datasets truly "finetuning
    proof"?

    '
  background:
    description: Most datasets get easier if models finetune on them, merely because
      of train/test overlap or because we amplify a subdistribution within the model
      that makes the model more confident in its already-known correct answers. Which
      datasets are most finetuning resistant? Are any of them finetuning proof?
  metadata:
    source: IdeaHub
    source_url: https://hypogenic.ai/ideahub/idea/XMu5uZXUuUig7I2khHhL
    idea_id: are_there_any_finetuning_proof_20251201_010302_86eeb0b1
    created_at: '2025-12-01T01:03:02.820107'
    status: submitted
    github_repo_name: finetuning-proof-datasets-5b00
    github_repo_url: https://github.com/ChicagoHAI/finetuning-proof-datasets-5b00
